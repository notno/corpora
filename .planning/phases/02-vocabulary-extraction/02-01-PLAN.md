---
phase: 02-vocabulary-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/corpora/models/vocabulary.py
  - src/corpora/extraction/__init__.py
  - src/corpora/extraction/extractor.py
  - src/corpora/extraction/filters.py
  - tests/test_extraction.py
autonomous: true
requirements_covered: [EXTRACT-01, EXTRACT-02, EXTRACT-03]

must_haves:
  truths:
    - "Nouns, verbs, and adjectives are extracted from text"
    - "Multi-word expressions (2-3 words) are extracted"
    - "Stopwords and common English words are filtered out"
    - "Extraction produces CandidateTerm objects with text, lemma, pos, and spans"
  artifacts:
    - path: "src/corpora/models/vocabulary.py"
      provides: "CandidateTerm, AxisScores, ClassifiedTerm Pydantic models"
      exports: ["CandidateTerm", "AxisScores", "ClassifiedTerm"]
    - path: "src/corpora/extraction/extractor.py"
      provides: "NLP-based term extraction"
      exports: ["TermExtractor", "extract_candidates"]
    - path: "src/corpora/extraction/filters.py"
      provides: "Stopword and common word filtering"
      exports: ["TermFilter"]
  key_links:
    - from: "src/corpora/extraction/extractor.py"
      to: "spacy"
      via: "nlp = spacy.load"
      pattern: "spacy\\.load"
    - from: "src/corpora/extraction/extractor.py"
      to: "src/corpora/extraction/filters.py"
      via: "filter import"
      pattern: "from.*filters.*import"
---

<objective>
Build the NLP-based term extraction pipeline that identifies fantasy-relevant vocabulary candidates from parsed document text.

Purpose: This is the first half of the hybrid NLP + Claude extraction approach. spaCy extracts candidates; later plans handle Claude classification.

Output:
- Pydantic models for vocabulary (CandidateTerm, AxisScores, ClassifiedTerm)
- Extraction module with spaCy-based term extraction
- Filtering for stopwords and common English words
- Tests verifying extraction quality
</objective>

<execution_context>
@C:\Users\nrosq\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\nrosq\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-vocabulary-extraction/02-CONTEXT.md
@.planning/phases/02-vocabulary-extraction/02-RESEARCH.md
@src/corpora/models/output.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create vocabulary models</name>
  <files>
    src/corpora/models/vocabulary.py
    src/corpora/models/__init__.py
  </files>
  <action>
Create Pydantic models for vocabulary extraction and classification:

1. **CandidateTerm** (extraction output):
   - text: str (original text)
   - lemma: str (normalized form)
   - pos: Literal["noun", "verb", "adjective", "phrase"]
   - source_span: tuple[int, int] (character offsets)

2. **AxisScores** (16-axis system from CONTEXT.md):
   - All 16 axes as float fields with Field(ge=0.0, le=1.0, default=0.0)
   - Elemental: fire, water, earth, air, light, shadow, life, void
   - Mechanical: force, binding, ward, sight, mind, time, space, fate

3. **ClassifiedTerm** (full schema from CLASS-01):
   - id: str (unique identifier like "src-fireball")
   - text: str (display text)
   - source: str (source document identifier)
   - genre: Literal["fantasy"] = "fantasy"
   - intent: str (offensive, defensive, utility, etc.)
   - pos: Literal["noun", "verb", "adjective", "phrase"]
   - axes: AxisScores
   - tags: List[str] = []
   - category: str (spell, creature, item, etc.)
   - canonical: str (normalized form)
   - mood: str (arcane, dark, heroic, etc.)
   - energy: str (energy type if applicable)
   - confidence: float = Field(ge=0.0, le=1.0)
   - secondary_intents: List[str] = []

Update models/__init__.py to export new models.

Follow project pattern: use Pydantic v2 with Field() for validation, same style as output.py.
  </action>
  <verify>
    python -c "from corpora.models import CandidateTerm, AxisScores, ClassifiedTerm; print('Models import OK')"
  </verify>
  <done>
    CandidateTerm, AxisScores, and ClassifiedTerm models exist with all required fields and validate correctly
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement term extraction with spaCy</name>
  <files>
    src/corpora/extraction/__init__.py
    src/corpora/extraction/extractor.py
    src/corpora/extraction/filters.py
  </files>
  <action>
Create the extraction module:

**filters.py** - Stopword and common word filtering:
```python
class TermFilter:
    def __init__(self):
        # Load spaCy stopwords
        # Add common English words (top ~1000 frequency)

    def should_keep(self, term: str) -> bool:
        # Return True if term is not a stopword/common word
        # Check: not in stopwords, length > 2, not all digits
```

**extractor.py** - spaCy-based extraction:
```python
class TermExtractor:
    def __init__(self):
        # Load spacy en_core_web_sm with NER disabled (not needed)
        # Initialize TermFilter

    def extract(self, text: str) -> List[CandidateTerm]:
        # Process text with spaCy
        # Extract single tokens: NOUN, VERB, ADJ
        # Extract noun chunks: 2-3 content words
        # Filter through TermFilter
        # Deduplicate by lemma
        # Return CandidateTerm objects
```

Key implementation notes from RESEARCH.md:
- Use `spacy.load("en_core_web_sm", disable=["ner"])` for speed
- Use `token.is_stop` for built-in stopword check
- Use `doc.noun_chunks` for multi-word expressions
- Filter chunk to content words (remove DET, stopwords)
- Track seen lemmas to deduplicate
- Store character offsets in source_span for traceability

**__init__.py** - Export public API:
```python
from .extractor import TermExtractor, extract_candidates
from .filters import TermFilter
```
  </action>
  <verify>
    python -c "from corpora.extraction import TermExtractor; e = TermExtractor(); print(f'Extractor ready, filter has {len(e.filter.stopwords)} stopwords')"
  </verify>
  <done>
    TermExtractor can process text and return CandidateTerm objects with nouns, verbs, adjectives, and 2-3 word phrases
  </done>
</task>

<task type="auto">
  <name>Task 3: Add extraction tests</name>
  <files>
    tests/test_extraction.py
  </files>
  <action>
Create tests for the extraction module:

```python
import pytest
from corpora.extraction import TermExtractor
from corpora.models import CandidateTerm

@pytest.fixture
def extractor():
    return TermExtractor()

def test_extracts_nouns(extractor):
    text = "The wizard cast a powerful fireball spell."
    terms = extractor.extract(text)
    lemmas = {t.lemma for t in terms}
    assert "wizard" in lemmas
    assert "fireball" in lemmas
    assert "spell" in lemmas

def test_extracts_verbs(extractor):
    text = "The knight attacked and defended with skill."
    terms = extractor.extract(text)
    lemmas = {t.lemma for t in terms}
    assert "attack" in lemmas or "attacked" in lemmas
    assert "defend" in lemmas or "defended" in lemmas

def test_extracts_adjectives(extractor):
    text = "The ancient dragon breathed deadly fire."
    terms = extractor.extract(text)
    lemmas = {t.lemma for t in terms}
    assert "ancient" in lemmas
    assert "deadly" in lemmas

def test_extracts_noun_chunks(extractor):
    text = "The dark elven warrior wielded a magical crystal sword."
    terms = extractor.extract(text)
    # Should find multi-word phrases
    phrases = [t for t in terms if t.pos == "phrase"]
    phrase_texts = {t.lemma for t in phrases}
    # Expect phrases like "dark elven warrior", "magical crystal sword"
    assert len(phrases) > 0

def test_filters_stopwords(extractor):
    text = "The wizard is very powerful and can do many things."
    terms = extractor.extract(text)
    lemmas = {t.lemma for t in terms}
    # Common stopwords should not appear
    assert "the" not in lemmas
    assert "is" not in lemmas
    assert "very" not in lemmas
    assert "and" not in lemmas
    assert "can" not in lemmas
    assert "do" not in lemmas

def test_deduplicates_by_lemma(extractor):
    text = "The wizards cast spells. A wizard casts a spell."
    terms = extractor.extract(text)
    lemmas = [t.lemma for t in terms]
    # Should not have duplicates
    assert len(lemmas) == len(set(lemmas))

def test_candidate_term_has_span(extractor):
    text = "A dragon appeared."
    terms = extractor.extract(text)
    dragon = next((t for t in terms if t.lemma == "dragon"), None)
    assert dragon is not None
    assert dragon.source_span[0] >= 0
    assert dragon.source_span[1] > dragon.source_span[0]
```

Run tests to verify extraction works correctly.
  </action>
  <verify>
    pytest tests/test_extraction.py -v
  </verify>
  <done>
    All extraction tests pass, confirming nouns/verbs/adjectives/phrases are extracted and stopwords are filtered
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Model validation:**
   ```bash
   python -c "
   from corpora.models import CandidateTerm, AxisScores, ClassifiedTerm
   # Test CandidateTerm
   ct = CandidateTerm(text='fireball', lemma='fireball', pos='noun', source_span=(0, 8))
   print(f'CandidateTerm: {ct.text}')
   # Test AxisScores
   axes = AxisScores(fire=0.9, force=0.7)
   print(f'AxisScores fire: {axes.fire}')
   # Test ClassifiedTerm
   term = ClassifiedTerm(
       id='test-1', text='Fireball', source='test', intent='offensive',
       pos='noun', axes=axes, category='spell', canonical='fireball',
       mood='arcane', energy='fire', confidence=0.95
   )
   print(f'ClassifiedTerm: {term.id}')
   "
   ```

2. **Extraction pipeline:**
   ```bash
   python -c "
   from corpora.extraction import TermExtractor
   e = TermExtractor()
   terms = e.extract('The wizard cast a fireball at the ancient dragon.')
   print(f'Extracted {len(terms)} terms:')
   for t in terms:
       print(f'  {t.lemma} ({t.pos})')
   "
   ```

3. **Tests pass:**
   ```bash
   pytest tests/test_extraction.py -v
   ```
</verification>

<success_criteria>
- spaCy en_core_web_sm model loads successfully
- Extraction returns CandidateTerm objects with text, lemma, pos, source_span
- Nouns, verbs, and adjectives are extracted (EXTRACT-02)
- Multi-word phrases (2-3 words) are extracted from noun chunks (EXTRACT-03)
- Stopwords and common words are filtered (EXTRACT-01 pre-filtering)
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-vocabulary-extraction/02-01-SUMMARY.md`
</output>
