---
phase: 03-output-ip-review
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/corpora/output/manifest.py
  - src/corpora/output/merger.py
  - src/corpora/output/consolidator.py
  - src/corpora/output/__init__.py
autonomous: true

must_haves:
  truths:
    - "User can consolidate multiple .vocab.json files into master.vocab.json"
    - "Duplicate terms are merged using confidence-weighted strategy"
    - "Backup is created before modifying master vocabulary"
    - "Change summary shows +new, ~updated, -removed counts"
  artifacts:
    - path: "src/corpora/output/manifest.py"
      provides: "CorporaManifest for tracking processed documents"
      exports: ["CorporaManifest", "ManifestEntry"]
    - path: "src/corpora/output/merger.py"
      provides: "merge_duplicates for confidence-weighted merging"
      exports: ["merge_duplicates", "ConsolidationSummary"]
    - path: "src/corpora/output/consolidator.py"
      provides: "consolidate_vocabularies for master file creation"
      exports: ["consolidate_vocabularies"]
  key_links:
    - from: "src/corpora/output/consolidator.py"
      to: "src/corpora/output/manifest.py"
      via: "uses CorporaManifest for change tracking"
      pattern: "CorporaManifest"
    - from: "src/corpora/output/consolidator.py"
      to: "src/corpora/output/merger.py"
      via: "uses merge_duplicates for deduplication"
      pattern: "merge_duplicates"
    - from: "src/corpora/output/consolidator.py"
      to: "src/corpora/ip/detector.py"
      via: "uses detect_ip for IP flagging"
      pattern: "detect_ip|flag_terms"
---

<objective>
Implement the consolidation pipeline: manifest-based change tracking, confidence-weighted duplicate merging, and master vocabulary generation with backup and change reporting.

Purpose: This enables OUTPUT-02 (consolidate into master), OUTPUT-03 (deduplicate with variant linking), and OUTPUT-04 (incremental updates).

Output: Manifest, merger, and consolidator modules for full consolidation workflow
</objective>

<execution_context>
@C:\Users\nrosq\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\nrosq\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-output-ip-review/03-CONTEXT.md
@.planning/phases/03-output-ip-review/03-RESEARCH.md

# Prior plans in this phase
@.planning/phases/03-output-ip-review/03-01-SUMMARY.md
@.planning/phases/03-output-ip-review/03-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create manifest module for change tracking</name>
  <files>
    src/corpora/output/manifest.py
    src/corpora/output/__init__.py
  </files>
  <action>
Create `manifest.py` with models and functions for `.corpora-manifest.json`:

1. `ManifestEntry` model:
   - source_path: str
   - source_hash: str (MD5 from compute_file_hash)
   - vocab_path: str
   - processed_at: datetime
   - term_count: int

2. `CorporaManifest` model:
   - schema_version: str = "1.0"
   - last_updated: datetime
   - documents: Dict[str, ManifestEntry] (keyed by source_path)

   Methods:
   - `needs_processing(source_path: Path) -> bool`: True if new or hash changed
   - `get_orphaned_vocabs(current_sources: List[Path]) -> List[str]`: vocab files without sources
   - `update_entry(source: Path, vocab: Path, term_count: int)`: Add/update manifest entry
   - `save(path: Path)`: Write to .corpora-manifest.json
   - `@classmethod load(path: Path) -> CorporaManifest`: Load or return empty manifest

Use compute_file_hash from vocab_writer.py for hash computation.

Add ManifestEntry, CorporaManifest to output/__init__.py exports.
  </action>
  <verify>
python -c "
from pathlib import Path
from corpora.output import CorporaManifest

# Test manifest operations
manifest = CorporaManifest()
test_source = Path('pyproject.toml')
print(f'Needs processing (new): {manifest.needs_processing(test_source)}')
manifest.update_entry(test_source, Path('test.vocab.json'), 10)
print(f'Needs processing (after update): {manifest.needs_processing(test_source)}')
print(f'Documents tracked: {len(manifest.documents)}')
"
  </verify>
  <done>
CorporaManifest tracks processed documents with content hashes, detects changes and orphans
  </done>
</task>

<task type="auto">
  <name>Task 2: Create merger module with confidence-weighted deduplication</name>
  <files>
    src/corpora/output/merger.py
    src/corpora/output/__init__.py
  </files>
  <action>
Create `merger.py` with:

1. `ConsolidationSummary` dataclass:
   - added: Set[str] (new canonical terms)
   - updated: Set[str] (terms with changed data)
   - removed: Set[str] (orphaned terms)
   - flagged: Set[str] (IP-flagged terms)
   - __str__(): Returns "+X new, ~Y updated, -Z removed, !W flagged"
   - is_empty(): True if no changes

2. `merge_duplicates(entries: List[VocabularyEntry]) -> VocabularyEntry`:
   Strategy (from RESEARCH.md):
   - If single entry, return as-is
   - Sort by confidence descending, use highest as base
   - Collect all sources with "; ".join()
   - Union all tags
   - Weighted average of axis scores (weighted by confidence)
   - Keep IP flag if any entry is flagged
   - Average confidence across entries

Handle AxisScores properly: if entry.axes is AxisScores object, use model_dump(), else use dict directly.

Add merge_duplicates, ConsolidationSummary to output/__init__.py exports.
  </action>
  <verify>
python -c "
from corpora.output.models import VocabularyEntry
from corpora.output import merge_duplicates, ConsolidationSummary

# Create test entries with different confidences
e1 = VocabularyEntry(
    id='a', text='Fireball', source='doc1.pdf', intent='offensive',
    pos='noun', category='spell', canonical='fireball', mood='arcane',
    confidence=0.9, tags=['evocation']
)
e2 = VocabularyEntry(
    id='b', text='Fireball', source='doc2.pdf', intent='offensive',
    pos='noun', category='spell', canonical='fireball', mood='fiery',
    confidence=0.7, tags=['fire']
)
merged = merge_duplicates([e1, e2])
print(f'Merged source: {merged.source}')
print(f'Merged tags: {merged.tags}')
print(f'Merged confidence: {merged.confidence}')

summary = ConsolidationSummary(added={'a', 'b'}, updated=set(), removed=set(), flagged={'c'})
print(f'Summary: {summary}')
"
  </verify>
  <done>
merge_duplicates combines entries with confidence weighting; ConsolidationSummary tracks changes
  </done>
</task>

<task type="auto">
  <name>Task 3: Create consolidator with backup and change summary</name>
  <files>
    src/corpora/output/consolidator.py
    src/corpora/output/__init__.py
  </files>
  <action>
Create `consolidator.py` with:

1. `backup_and_write(path: Path, content: str) -> Optional[Path]`:
   - If path exists, create timestamped .bak AND simple .bak
   - Write to .tmp file first, then atomic replace
   - Return backup path or None

2. `consolidate_vocabularies(vocab_files: List[Path], master_path: Path, blocklist: Optional[IPBlocklist] = None) -> ConsolidationSummary`:
   - Load existing master if present (for change detection)
   - Load all vocab files and group entries by canonical form
   - Apply merge_duplicates to each group
   - Apply IP detection if blocklist provided
   - Track added/updated/removed/flagged
   - Use backup_and_write for safe master update
   - Sort entries by canonical for consistent output
   - Return ConsolidationSummary

For master metadata:
- source_path: "consolidated"
- source_hash: "" (not applicable)
- term_count, classified_count, flagged_count from merged entries

Add consolidate_vocabularies, backup_and_write to output/__init__.py exports.
  </action>
  <verify>
python -c "
from pathlib import Path
from corpora.output import backup_and_write

# Test backup_and_write with temp file
test_path = Path('test_backup.json')
test_path.write_text('{\"old\": true}')
backup = backup_and_write(test_path, '{\"new\": true}')
print(f'Backup created: {backup}')
assert test_path.read_text() == '{\"new\": true}'
# Cleanup
test_path.unlink()
if backup:
    backup.unlink()
    Path(str(test_path) + '.bak').unlink(missing_ok=True)
print('backup_and_write works correctly')
"
  </verify>
  <done>
consolidate_vocabularies merges vocab files into master.vocab.json with backup, deduplication, and change summary
  </done>
</task>

</tasks>

<verification>
Full consolidation test:
```bash
python -c "
from pathlib import Path
from corpora.output import (
    VocabularyOutput, VocabularyEntry, VocabularyMetadata,
    consolidate_vocabularies, CorporaManifest
)
import tempfile
import json

with tempfile.TemporaryDirectory() as tmpdir:
    tmp = Path(tmpdir)

    # Create two vocab files with overlapping terms
    v1 = VocabularyOutput(
        metadata=VocabularyMetadata(source_path='doc1.pdf', source_hash='abc', term_count=1, classified_count=1),
        entries=[VocabularyEntry(
            id='1', text='Fireball', source='doc1', intent='offensive',
            pos='noun', category='spell', canonical='fireball', mood='arcane', confidence=0.9
        )]
    )
    v1.to_file(tmp / 'doc1.vocab.json')

    v2 = VocabularyOutput(
        metadata=VocabularyMetadata(source_path='doc2.pdf', source_hash='def', term_count=2, classified_count=2),
        entries=[
            VocabularyEntry(
                id='2', text='Fireball', source='doc2', intent='offensive',
                pos='noun', category='spell', canonical='fireball', mood='fiery', confidence=0.7
            ),
            VocabularyEntry(
                id='3', text='Lightning', source='doc2', intent='offensive',
                pos='noun', category='spell', canonical='lightning', mood='electric', confidence=0.8
            )
        ]
    )
    v2.to_file(tmp / 'doc2.vocab.json')

    # Consolidate
    summary = consolidate_vocabularies(
        [tmp / 'doc1.vocab.json', tmp / 'doc2.vocab.json'],
        tmp / 'master.vocab.json'
    )

    print(f'Consolidation: {summary}')

    # Verify master
    with open(tmp / 'master.vocab.json') as f:
        master = json.load(f)
    print(f'Master has {len(master[\"entries\"])} terms (expected 2: fireball merged, lightning new)')
"
```
</verification>

<success_criteria>
- CorporaManifest tracks documents with content hashes
- needs_processing returns True for new/changed files
- merge_duplicates uses confidence-weighted strategy
- Multiple sources joined with "; "
- Tags unioned from all entries
- consolidate_vocabularies creates master.vocab.json
- Backup created before modifying existing master
- Change summary shows accurate +/-/~ counts
- IP flags preserved through consolidation
</success_criteria>

<output>
After completion, create `.planning/phases/03-output-ip-review/03-03-SUMMARY.md`
</output>
