---
phase: 02-vocabulary-extraction
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/corpora/classification/__init__.py
  - src/corpora/classification/prompts.py
  - src/corpora/classification/client.py
  - src/corpora/classification/batch.py
  - tests/test_classification.py
autonomous: true
requirements_covered: [CLASS-01, CLASS-02, CLASS-03, CLASS-04, CLASS-05]

must_haves:
  truths:
    - "Claude API can classify terms with full 16-axis schema"
    - "Batch API is used for cost-efficient bulk classification"
    - "System prompt is cached to reduce token costs"
    - "Rate limits trigger automatic exponential backoff"
    - "Classification output validates against Pydantic schema"
  artifacts:
    - path: "src/corpora/classification/prompts.py"
      provides: "Cacheable system prompt with 16-axis definitions"
      exports: ["CLASSIFICATION_SYSTEM_PROMPT", "build_user_prompt"]
    - path: "src/corpora/classification/client.py"
      provides: "Claude API wrapper with retry logic"
      exports: ["ClassificationClient"]
    - path: "src/corpora/classification/batch.py"
      provides: "Batch API handling for bulk classification"
      exports: ["BatchClassifier", "create_batch", "poll_batch", "stream_results"]
  key_links:
    - from: "src/corpora/classification/client.py"
      to: "anthropic"
      via: "anthropic.Anthropic() client"
      pattern: "anthropic\\.Anthropic"
    - from: "src/corpora/classification/client.py"
      to: "tenacity"
      via: "@retry decorator"
      pattern: "@retry"
    - from: "src/corpora/classification/batch.py"
      to: "src/corpora/classification/prompts.py"
      via: "prompt import for batch requests"
      pattern: "from.*prompts.*import"
---

<objective>
Build the Claude API classification infrastructure with Batch API support and prompt caching.

Purpose: This is the classification half of the hybrid extraction pipeline. Terms from Plan 01 will be classified here using Claude Haiku 4.5 via Batch API.

Output:
- Classification prompts designed for caching (>1024 tokens)
- Claude API client with tenacity-based retry logic
- Batch API handling for cost-efficient processing
- Tests verifying API integration (mocked for CI)
</objective>

<execution_context>
@C:\Users\nrosq\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\nrosq\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-vocabulary-extraction/02-CONTEXT.md
@.planning/phases/02-vocabulary-extraction/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create classification prompts</name>
  <files>
    src/corpora/classification/__init__.py
    src/corpora/classification/prompts.py
  </files>
  <action>
Create the classification prompts module with a system prompt designed for caching:

**prompts.py:**

The system prompt MUST be >1024 tokens to enable prompt caching on Claude Haiku/Sonnet. Include:

1. **Role definition**: You are a fantasy vocabulary classifier for game development
2. **Full 16-axis system definitions** (from CONTEXT.md):
   - Elemental axes (0-7): fire, water, earth, air, light, shadow, life, void
   - Mechanical axes (8-15): force, binding, ward, sight, mind, time, space, fate
   - Include descriptions for each axis (heat/destruction/passion for fire, etc.)
3. **Output schema**: JSON with all ClassifiedTerm fields
4. **Classification guidelines**:
   - How to determine intent (offensive, defensive, utility, etc.)
   - How to assign mood (arcane, dark, heroic, etc.)
   - How to identify category (spell, creature, item, location, etc.)
   - How to score axes (0.0-1.0, most terms should have 2-4 non-zero axes)

```python
CLASSIFICATION_SYSTEM_PROMPT = """You are a fantasy vocabulary classifier...
[Full detailed prompt - aim for 1200-1500 tokens]
"""

def build_user_prompt(term: str, context: str = "", lemma: str = "", pos: str = "") -> str:
    """Build the user message for classifying a single term.

    Args:
        term: The term to classify
        context: Optional surrounding text for context
        lemma: Optional lemma (normalized form) to aid canonical form generation
        pos: Optional part of speech from NLP extraction

    Returns:
        User prompt string
    """
    parts = [f"Classify this fantasy term: '{term}'"]
    if lemma and lemma != term.lower():
        parts.append(f"Lemma: {lemma}")
    if pos:
        parts.append(f"POS: {pos}")
    if context:
        parts.append(f"Context: {context}")
    return "\n".join(parts)

def build_batch_user_prompt(terms: list[str]) -> str:
    """Build user prompt for batch classification (multiple terms).

    Args:
        terms: List of terms to classify (recommend 10-20 per request)

    Returns:
        User prompt string requesting JSON array output
    """
    terms_list = "\n".join(f"- {term}" for term in terms)
    return f"Classify these fantasy terms. Return a JSON array:\n\n{terms_list}"
```

**__init__.py:**
```python
from .prompts import CLASSIFICATION_SYSTEM_PROMPT, build_user_prompt, build_batch_user_prompt
from .client import ClassificationClient
from .batch import BatchClassifier
```
  </action>
  <verify>
    python -c "from corpora.classification.prompts import CLASSIFICATION_SYSTEM_PROMPT; print(f'System prompt: {len(CLASSIFICATION_SYSTEM_PROMPT)} chars, ~{len(CLASSIFICATION_SYSTEM_PROMPT.split())} words')"
  </verify>
  <done>
    System prompt exists with >1024 tokens (approximately 300+ words) and includes full 16-axis definitions
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Claude API client with retry logic</name>
  <files>
    src/corpora/classification/client.py
  </files>
  <action>
Create the Claude API client wrapper with tenacity-based retry logic:

```python
"""Claude API client for term classification."""

import json
from typing import Optional

import anthropic
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from corpora.models import ClassifiedTerm
from .prompts import CLASSIFICATION_SYSTEM_PROMPT, build_user_prompt


class ClassificationClient:
    """Claude API client for classifying fantasy vocabulary."""

    MODEL = "claude-haiku-4-5-20250929"  # Cost-effective, 90% quality
    MAX_TOKENS = 2048

    def __init__(self, api_key: Optional[str] = None):
        """Initialize the client.

        Args:
            api_key: Anthropic API key. If None, uses ANTHROPIC_API_KEY env var.
        """
        self.client = anthropic.Anthropic(api_key=api_key)

    @retry(
        retry=retry_if_exception_type(anthropic.RateLimitError),
        wait=wait_exponential(multiplier=2, min=1, max=120),
        stop=stop_after_attempt(5),
    )
    def classify_term(
        self,
        term: str,
        source: str,
        context: str = "",
    ) -> ClassifiedTerm:
        """Classify a single term using Claude API.

        Args:
            term: The term to classify
            source: Source document identifier
            context: Optional surrounding text

        Returns:
            ClassifiedTerm with full classification

        Raises:
            anthropic.RateLimitError: After 5 retries with backoff
            ValueError: If response cannot be parsed
        """
        response = self.client.messages.create(
            model=self.MODEL,
            max_tokens=self.MAX_TOKENS,
            system=[
                {
                    "type": "text",
                    "text": CLASSIFICATION_SYSTEM_PROMPT,
                    "cache_control": {"type": "ephemeral"},
                }
            ],
            messages=[
                {"role": "user", "content": build_user_prompt(term, context)}
            ],
        )

        # Parse JSON response
        content = response.content[0].text
        try:
            data = json.loads(content)
            # Add source if not in response
            data["source"] = source
            return ClassifiedTerm.model_validate(data)
        except (json.JSONDecodeError, Exception) as e:
            raise ValueError(f"Failed to parse classification for '{term}': {e}")

    def estimate_cost(
        self,
        num_terms: int,
        use_batch: bool = True,
    ) -> dict:
        """Estimate API cost for classification.

        Args:
            num_terms: Number of terms to classify
            use_batch: Whether using Batch API (50% discount)

        Returns:
            Dict with estimated input/output tokens and cost
        """
        # Estimates based on RESEARCH.md
        est_input_tokens = num_terms * 200  # ~200 per term prompt
        est_output_tokens = num_terms * 500  # ~500 per classification

        # Haiku 4.5 pricing (per million tokens)
        input_price = 0.80  # $0.80/MTok input
        output_price = 4.00  # $4.00/MTok output

        if use_batch:
            input_price *= 0.5  # 50% batch discount
            output_price *= 0.5

        # First request doesn't benefit from cache, subsequent do
        # Assume 90% cache hit rate after first
        cache_savings = 0.9 * 0.9  # 90% of requests * 90% savings
        effective_input = est_input_tokens * (1 - cache_savings)

        cost = (effective_input * input_price + est_output_tokens * output_price) / 1_000_000

        return {
            "num_terms": num_terms,
            "est_input_tokens": est_input_tokens,
            "est_output_tokens": est_output_tokens,
            "use_batch": use_batch,
            "est_cost_usd": round(cost, 4),
        }
```

Key implementation notes:
- Use claude-haiku-4-5-20250929 (recommended in RESEARCH.md for cost/quality balance)
- Use cache_control on system prompt for 90% input cost savings
- Use tenacity @retry decorator for rate limit handling (CLASS-03)
- Parse JSON response into ClassifiedTerm model (CLASS-02 validation)
  </action>
  <verify>
    python -c "from corpora.classification.client import ClassificationClient; c = ClassificationClient.__new__(ClassificationClient); print(f'Client model: {c.MODEL}')"
  </verify>
  <done>
    ClassificationClient exists with retry logic, prompt caching, and cost estimation
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement Batch API handling</name>
  <files>
    src/corpora/classification/batch.py
    tests/test_classification.py
  </files>
  <action>
Create the Batch API handler for cost-efficient bulk classification:

**batch.py:**
```python
"""Batch API handling for bulk term classification."""

import json
import time
from typing import Iterator, List, Optional, Callable

import anthropic
from anthropic.types.message_create_params import MessageCreateParamsNonStreaming
from anthropic.types.messages.batch_create_params import Request

from corpora.models import ClassifiedTerm
from .prompts import CLASSIFICATION_SYSTEM_PROMPT, build_user_prompt


class BatchClassifier:
    """Batch API classifier for cost-efficient processing."""

    MODEL = "claude-haiku-4-5-20250929"
    MAX_TOKENS = 2048
    DEFAULT_BATCH_SIZE = 50  # Terms per batch request

    def __init__(self, api_key: Optional[str] = None):
        self.client = anthropic.Anthropic(api_key=api_key)

    def create_batch(
        self,
        terms: List[tuple[str, str, str, str]],  # (term, source, lemma, pos) tuples
    ) -> str:
        """Create a batch request for term classification.

        Args:
            terms: List of (term, source, lemma, pos) tuples to classify

        Returns:
            Batch ID for tracking
        """
        requests = []
        for i, (term, source, lemma, pos) in enumerate(terms):
            requests.append(
                Request(
                    custom_id=f"term-{i}-{source}",
                    params=MessageCreateParamsNonStreaming(
                        model=self.MODEL,
                        max_tokens=self.MAX_TOKENS,
                        system=[
                            {
                                "type": "text",
                                "text": CLASSIFICATION_SYSTEM_PROMPT,
                                "cache_control": {"type": "ephemeral"},
                            }
                        ],
                        messages=[
                            {"role": "user", "content": build_user_prompt(term, lemma=lemma, pos=pos)}
                        ],
                    ),
                )
            )

        batch = self.client.messages.batches.create(requests=requests)
        return batch.id

    def poll_batch(
        self,
        batch_id: str,
        poll_interval: int = 60,
        on_progress: Optional[Callable[[int, int], None]] = None,
    ) -> None:
        """Poll until batch processing completes.

        Args:
            batch_id: Batch ID to poll
            poll_interval: Seconds between polls
            on_progress: Optional callback(completed, total)
        """
        while True:
            batch = self.client.messages.batches.retrieve(batch_id)
            counts = batch.request_counts

            if on_progress:
                completed = counts.succeeded + counts.errored + counts.expired + counts.canceled
                on_progress(completed, completed + counts.processing)

            if batch.processing_status == "ended":
                return

            time.sleep(poll_interval)

    def stream_results(
        self,
        batch_id: str,
        source: str,
    ) -> Iterator[tuple[int, ClassifiedTerm | dict]]:
        """Stream batch results as ClassifiedTerm objects.

        Args:
            batch_id: Batch ID to retrieve results from
            source: Source document identifier for all terms

        Yields:
            Tuples of (index, ClassifiedTerm) for success
            Tuples of (index, {"error": str}) for failures
        """
        for result in self.client.messages.batches.results(batch_id):
            # Extract index from custom_id "term-{i}-{source}"
            parts = result.custom_id.split("-")
            idx = int(parts[1]) if len(parts) >= 2 else 0

            if result.result.type == "succeeded":
                content = result.result.message.content[0].text
                try:
                    data = json.loads(content)
                    data["source"] = source
                    term = ClassifiedTerm.model_validate(data)
                    yield (idx, term)
                except Exception as e:
                    yield (idx, {"error": f"Parse error: {e}"})

            elif result.result.type == "errored":
                yield (idx, {"error": str(result.result.error)})

            elif result.result.type == "expired":
                yield (idx, {"error": "Request expired"})

            elif result.result.type == "canceled":
                yield (idx, {"error": "Request canceled"})

    def get_batch_status(self, batch_id: str) -> dict:
        """Get current batch status.

        Args:
            batch_id: Batch ID to check

        Returns:
            Dict with processing_status and request_counts
        """
        batch = self.client.messages.batches.retrieve(batch_id)
        return {
            "id": batch.id,
            "status": batch.processing_status,
            "counts": {
                "processing": batch.request_counts.processing,
                "succeeded": batch.request_counts.succeeded,
                "errored": batch.request_counts.errored,
                "expired": batch.request_counts.expired,
                "canceled": batch.request_counts.canceled,
            },
        }
```

**tests/test_classification.py:**
Create tests with mocked API for CI:

```python
import pytest
from unittest.mock import Mock, patch, MagicMock
import json

from corpora.classification import ClassificationClient, BatchClassifier
from corpora.classification.prompts import CLASSIFICATION_SYSTEM_PROMPT
from corpora.models import ClassifiedTerm, AxisScores


class TestPrompts:
    def test_system_prompt_length(self):
        # Must be >1024 tokens for caching (roughly 4 chars/token)
        assert len(CLASSIFICATION_SYSTEM_PROMPT) > 4000, "System prompt too short for caching"

    def test_system_prompt_contains_axes(self):
        for axis in ["fire", "water", "earth", "air", "light", "shadow", "life", "void",
                     "force", "binding", "ward", "sight", "mind", "time", "space", "fate"]:
            assert axis in CLASSIFICATION_SYSTEM_PROMPT.lower(), f"Missing axis: {axis}"


class TestClassificationClient:
    @patch("corpora.classification.client.anthropic.Anthropic")
    def test_classify_term_parses_response(self, mock_anthropic):
        # Mock response
        mock_response = Mock()
        mock_response.content = [Mock(text=json.dumps({
            "id": "test-fireball",
            "text": "Fireball",
            "genre": "fantasy",
            "intent": "offensive",
            "pos": "noun",
            "axes": {"fire": 0.9, "force": 0.7},
            "tags": ["evocation"],
            "category": "spell",
            "canonical": "fireball",
            "mood": "arcane",
            "energy": "fire",
            "confidence": 0.95,
        }))]

        mock_client = Mock()
        mock_client.messages.create.return_value = mock_response
        mock_anthropic.return_value = mock_client

        client = ClassificationClient()
        result = client.classify_term("fireball", source="test")

        assert isinstance(result, ClassifiedTerm)
        assert result.text == "Fireball"
        assert result.axes.fire == 0.9
        assert result.source == "test"

    def test_estimate_cost(self):
        client = ClassificationClient.__new__(ClassificationClient)
        estimate = client.estimate_cost(100, use_batch=True)

        assert "est_cost_usd" in estimate
        assert estimate["num_terms"] == 100
        assert estimate["use_batch"] is True
        assert estimate["est_cost_usd"] > 0


class TestBatchClassifier:
    @patch("corpora.classification.batch.anthropic.Anthropic")
    def test_create_batch(self, mock_anthropic):
        mock_batch = Mock()
        mock_batch.id = "batch_123"

        mock_client = Mock()
        mock_client.messages.batches.create.return_value = mock_batch
        mock_anthropic.return_value = mock_client

        classifier = BatchClassifier()
        batch_id = classifier.create_batch([("fireball", "test", "fireball", "noun"), ("dragon", "test", "dragon", "noun")])

        assert batch_id == "batch_123"
        mock_client.messages.batches.create.assert_called_once()
```
  </action>
  <verify>
    pytest tests/test_classification.py -v
  </verify>
  <done>
    BatchClassifier can create batches, poll status, and stream results; all tests pass with mocked API
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Prompt caching eligibility:**
   ```bash
   python -c "
   from corpora.classification.prompts import CLASSIFICATION_SYSTEM_PROMPT
   words = len(CLASSIFICATION_SYSTEM_PROMPT.split())
   # ~4 chars per token on average
   est_tokens = len(CLASSIFICATION_SYSTEM_PROMPT) / 4
   print(f'System prompt: {words} words, ~{int(est_tokens)} tokens')
   print(f'Caching eligible (>1024 tokens): {est_tokens > 1024}')
   "
   ```

2. **Client initialization:**
   ```bash
   python -c "
   from corpora.classification import ClassificationClient, BatchClassifier
   # Just verify classes exist and can be imported
   print(f'Client model: {ClassificationClient.MODEL}')
   print(f'Batch model: {BatchClassifier.MODEL}')
   "
   ```

3. **Cost estimation:**
   ```bash
   python -c "
   from corpora.classification.client import ClassificationClient
   c = ClassificationClient.__new__(ClassificationClient)
   est = c.estimate_cost(500, use_batch=True)
   print(f'500 terms batch estimate: \${est[\"est_cost_usd\"]:.4f}')
   "
   ```

4. **All tests pass:**
   ```bash
   pytest tests/test_classification.py -v
   ```
</verification>

<success_criteria>
- System prompt is >1024 tokens with full 16-axis definitions (CLASS-05 caching)
- ClassificationClient has @retry decorator for rate limits (CLASS-03)
- BatchClassifier can create batches and stream results (CLASS-04)
- Classification output validates against ClassifiedTerm schema (CLASS-02)
- Cost estimation works for preview mode
- All tests pass with mocked API
</success_criteria>

<output>
After completion, create `.planning/phases/02-vocabulary-extraction/02-02-SUMMARY.md`
</output>
