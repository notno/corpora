---
phase: 04-batch-processing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/corpora/batch/__init__.py
  - src/corpora/batch/models.py
  - src/corpora/batch/processor.py
autonomous: true

must_haves:
  truths:
    - "Multiple documents process in parallel using ThreadPoolExecutor"
    - "Failed documents retry once before marking as failed"
    - "Each document processing result includes success/failure status and term count"
    - "Manifest is updated after EACH document completes (not at end)"
  artifacts:
    - path: "src/corpora/batch/models.py"
      provides: "BatchConfig and DocumentResult models"
      contains: "class DocumentResult"
    - path: "src/corpora/batch/processor.py"
      provides: "BatchProcessor class with parallel execution"
      contains: "ThreadPoolExecutor"
  key_links:
    - from: "src/corpora/batch/processor.py"
      to: "src/corpora/output/manifest.py"
      via: "CorporaManifest.needs_processing and update_entry"
      pattern: "manifest\\.(needs_processing|update_entry)"
    - from: "src/corpora/batch/processor.py"
      to: "concurrent.futures"
      via: "ThreadPoolExecutor.submit and as_completed"
      pattern: "ThreadPoolExecutor|as_completed"
---

<objective>
Create the core batch processing module with parallel document execution, retry logic, and manifest-based resumability.

Purpose: This is the engine that processes multiple documents in parallel while handling failures gracefully and updating the manifest after each document for safe interruption.

Output: src/corpora/batch/ module with BatchProcessor class that can process a list of document paths using ThreadPoolExecutor
</objective>

<execution_context>
@C:\Users\nrosq\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\nrosq\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-batch-processing/04-CONTEXT.md
@.planning/phases/04-batch-processing/04-RESEARCH.md
@src/corpora/output/manifest.py
@src/corpora/cli/extract.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batch models for configuration and results</name>
  <files>src/corpora/batch/__init__.py, src/corpora/batch/models.py</files>
  <action>
Create src/corpora/batch/ package with models:

1. In models.py, create:

```python
from enum import Enum
from pathlib import Path
from typing import Optional, List
from pydantic import BaseModel, Field

class DocumentStatus(str, Enum):
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"  # Already processed, unchanged

class DocumentResult(BaseModel):
    """Result of processing a single document."""
    source_path: Path
    status: DocumentStatus
    term_count: int = 0
    vocab_path: Optional[Path] = None
    error: Optional[str] = None
    duration_seconds: float = 0.0

class BatchConfig(BaseModel):
    """Configuration for batch processing."""
    input_dir: Path
    output_dir: Path
    max_workers: int = Field(default=4, ge=1, le=16)
    force_reprocess: bool = False
    blocklist_path: Optional[Path] = None

class BatchSummary(BaseModel):
    """Summary of batch processing run."""
    total_documents: int
    processed: int
    skipped: int
    failed: int
    total_terms: int
    duration_seconds: float
    errors: List[str] = Field(default_factory=list)
```

2. In __init__.py, export: DocumentStatus, DocumentResult, BatchConfig, BatchSummary

Use sysexits.h convention for exit codes:
- EXIT_SUCCESS = 0
- EXIT_PARTIAL = 75 (EX_TEMPFAIL - some failed)
- EXIT_NO_INPUT = 66 (no documents found)
  </action>
  <verify>python -c "from corpora.batch import DocumentResult, BatchConfig, BatchSummary; print('Models OK')"</verify>
  <done>Batch models importable and Pydantic validation works</done>
</task>

<task type="auto">
  <name>Task 2: Implement BatchProcessor with parallel execution and retry</name>
  <files>src/corpora/batch/processor.py, src/corpora/batch/__init__.py</files>
  <action>
Create BatchProcessor class in processor.py:

```python
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Callable, Iterator, List, Optional

from corpora.batch.models import BatchConfig, DocumentResult, DocumentStatus, BatchSummary
from corpora.output.manifest import CorporaManifest

# Supported file extensions
SUPPORTED_EXTENSIONS = {'.pdf', '.epub'}

class BatchProcessor:
    """Processes multiple documents in parallel with fault tolerance."""

    def __init__(
        self,
        config: BatchConfig,
        manifest_path: Optional[Path] = None,
        on_document_complete: Optional[Callable[[DocumentResult], None]] = None,
    ):
        self.config = config
        self.manifest_path = manifest_path or (config.output_dir / ".corpora-manifest.json")
        self.manifest = CorporaManifest.load(self.manifest_path)
        self.on_document_complete = on_document_complete

    def discover_documents(self) -> List[Path]:
        """Find all supported documents in input directory."""
        documents = []
        for ext in SUPPORTED_EXTENSIONS:
            documents.extend(self.config.input_dir.glob(f"*{ext}"))
        return sorted(documents)

    def _process_single_document(self, source: Path) -> DocumentResult:
        """Process a single document through the full pipeline.

        Pipeline: parse -> extract -> output
        Returns DocumentResult with status and term count.
        """
        start_time = time.time()

        # Import here to avoid circular imports
        from corpora.parsers import PDFParser, EPUBParser
        from corpora.extraction import TermExtractor
        from corpora.classification import ClassificationClient
        from corpora.output import VocabularyWriter
        from corpora.ip import IPBlocklist

        try:
            # 1. Parse document
            ext = source.suffix.lower()
            if ext == '.pdf':
                parser = PDFParser()
            elif ext == '.epub':
                parser = EPUBParser()
            else:
                raise ValueError(f"Unsupported format: {ext}")

            doc_output = parser.parse(source)

            # 2. Extract terms
            text_parts = [block.text for block in doc_output.content if block.text]
            full_text = "\n\n".join(text_parts)

            if not full_text.strip():
                return DocumentResult(
                    source_path=source,
                    status=DocumentStatus.SUCCESS,
                    term_count=0,
                    duration_seconds=time.time() - start_time,
                )

            extractor = TermExtractor()
            candidates = extractor.extract(full_text)

            if not candidates:
                return DocumentResult(
                    source_path=source,
                    status=DocumentStatus.SUCCESS,
                    term_count=0,
                    duration_seconds=time.time() - start_time,
                )

            # 3. Classify terms (sync API for parallel-friendliness)
            client = ClassificationClient()
            classified_terms = []
            for term in candidates:
                try:
                    result = client.classify_term(
                        term=term.text,
                        source=doc_output.source,
                        lemma=term.lemma,
                        pos=term.pos,
                    )
                    classified_terms.append(result)
                except Exception:
                    pass  # Skip failed terms, continue

            # 4. Write vocabulary file with IP detection
            blocklist = None
            if self.config.blocklist_path and self.config.blocklist_path.exists():
                blocklist = IPBlocklist(self.config.blocklist_path)

            writer = VocabularyWriter(blocklist=blocklist)
            vocab_path = self.config.output_dir / f"{source.stem}.vocab.json"
            self.config.output_dir.mkdir(parents=True, exist_ok=True)

            vocab_output = writer.write(
                terms=classified_terms,
                source=doc_output.source,
                output_path=vocab_path,
            )

            return DocumentResult(
                source_path=source,
                status=DocumentStatus.SUCCESS,
                term_count=len(vocab_output.entries),
                vocab_path=vocab_path,
                duration_seconds=time.time() - start_time,
            )

        except Exception as e:
            return DocumentResult(
                source_path=source,
                status=DocumentStatus.FAILED,
                error=str(e),
                duration_seconds=time.time() - start_time,
            )

    def _process_with_retry(self, source: Path) -> DocumentResult:
        """Process document with one retry on failure."""
        result = self._process_single_document(source)
        if result.status == DocumentStatus.FAILED:
            # Retry once
            result = self._process_single_document(source)
        return result

    def process(self) -> Iterator[DocumentResult]:
        """Process all documents, yielding results as they complete.

        Uses ThreadPoolExecutor for parallel processing.
        Updates manifest after EACH document completes.
        """
        documents = self.discover_documents()

        if not documents:
            return

        # Determine which documents need processing
        to_process = []
        for doc in documents:
            if self.config.force_reprocess or self.manifest.needs_processing(doc):
                to_process.append(doc)
            else:
                # Yield skipped result immediately
                result = DocumentResult(
                    source_path=doc,
                    status=DocumentStatus.SKIPPED,
                )
                if self.on_document_complete:
                    self.on_document_complete(result)
                yield result

        if not to_process:
            return

        # Process in parallel
        max_workers = min(self.config.max_workers, os.cpu_count() or 4)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_doc = {
                executor.submit(self._process_with_retry, doc): doc
                for doc in to_process
            }

            for future in as_completed(future_to_doc):
                doc = future_to_doc[future]
                try:
                    result = future.result()
                except Exception as e:
                    result = DocumentResult(
                        source_path=doc,
                        status=DocumentStatus.FAILED,
                        error=str(e),
                    )

                # Update manifest IMMEDIATELY after each document
                if result.status == DocumentStatus.SUCCESS and result.vocab_path:
                    self.manifest.update_entry(
                        source=result.source_path,
                        vocab=result.vocab_path,
                        term_count=result.term_count,
                    )
                    self.manifest.save(self.manifest_path)

                if self.on_document_complete:
                    self.on_document_complete(result)

                yield result

    def run(self) -> BatchSummary:
        """Process all documents and return summary."""
        start_time = time.time()
        results = list(self.process())

        processed = sum(1 for r in results if r.status == DocumentStatus.SUCCESS)
        skipped = sum(1 for r in results if r.status == DocumentStatus.SKIPPED)
        failed = sum(1 for r in results if r.status == DocumentStatus.FAILED)
        total_terms = sum(r.term_count for r in results)
        errors = [r.error for r in results if r.error]

        return BatchSummary(
            total_documents=len(results),
            processed=processed,
            skipped=skipped,
            failed=failed,
            total_terms=total_terms,
            duration_seconds=time.time() - start_time,
            errors=errors,
        )
```

Key implementation details:
- Uses sync API (not batch API) for parallel-friendliness - each worker makes its own API calls
- tenacity is already configured in ClassificationClient for rate limiting
- Manifest updated AFTER EACH document (Ctrl+C safe)
- Retry once on failure before marking failed
- on_document_complete callback for progress updates

Update __init__.py to export BatchProcessor.
  </action>
  <verify>python -c "from corpora.batch import BatchProcessor, BatchConfig; print('Processor OK')"</verify>
  <done>BatchProcessor can process documents in parallel with retry logic and manifest updates</done>
</task>

</tasks>

<verification>
1. Models validate: `python -c "from corpora.batch import *; print('All exports OK')"`
2. BatchProcessor instantiates with config
3. Module structure is clean with no circular imports
</verification>

<success_criteria>
- src/corpora/batch/ package exists with models.py, processor.py, __init__.py
- DocumentResult, BatchConfig, BatchSummary models work with Pydantic
- BatchProcessor.process() yields DocumentResult for each document
- BatchProcessor uses ThreadPoolExecutor for parallel execution
- Manifest is updated after each document completes
- Failed documents are retried once
</success_criteria>

<output>
After completion, create `.planning/phases/04-batch-processing/04-01-SUMMARY.md`
</output>
