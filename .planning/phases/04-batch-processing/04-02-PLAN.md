---
phase: 04-batch-processing
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/corpora/cli/batch.py
  - src/corpora/cli/main.py
  - tests/test_batch.py
autonomous: true

must_haves:
  truths:
    - "User can run 'corpora batch <folder>' to process all documents"
    - "Progress bar shows documents completed vs remaining during processing"
    - "End summary shows total docs, terms, time, and any errors"
    - "Interrupted runs resume automatically (skip already-processed docs)"
    - "--force flag reprocesses all documents ignoring manifest"
    - "--quiet flag shows only errors and final summary"
  artifacts:
    - path: "src/corpora/cli/batch.py"
      provides: "batch_command CLI implementation"
      contains: "def batch_command"
    - path: "tests/test_batch.py"
      provides: "Test coverage for batch module"
      contains: "def test_"
  key_links:
    - from: "src/corpora/cli/batch.py"
      to: "src/corpora/batch/processor.py"
      via: "BatchProcessor instantiation and run()"
      pattern: "BatchProcessor\\("
    - from: "src/corpora/cli/main.py"
      to: "src/corpora/cli/batch.py"
      via: "app.command registration"
      pattern: "batch_command"
---

<objective>
Implement the `corpora batch` CLI command with Rich progress display, quiet mode, and comprehensive tests.

Purpose: This is the user-facing interface for batch processing. Users point it at a folder and watch their documents process with a progress bar.

Output: Working `corpora batch` command with --workers, --quiet, --force flags and full test coverage
</objective>

<execution_context>
@C:\Users\nrosq\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\nrosq\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-batch-processing/04-CONTEXT.md
@.planning/phases/04-batch-processing/04-RESEARCH.md
@src/corpora/cli/main.py
@src/corpora/cli/extract.py
@src/corpora/batch/processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement batch CLI command with progress display</name>
  <files>src/corpora/cli/batch.py, src/corpora/cli/main.py</files>
  <action>
Create src/corpora/cli/batch.py with batch_command:

```python
"""Batch processing subcommand for folder-level document processing.

Implements the `corpora batch` command for processing entire folders:
- Discovers all PDF/EPUB files in input directory
- Processes in parallel with configurable workers
- Shows Rich progress bar with document-level updates
- Resumes interrupted runs via manifest
- Outputs per-document .vocab.json files
"""

import os
import sys
from datetime import timedelta
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn

from corpora.batch import BatchProcessor, BatchConfig, DocumentResult, DocumentStatus, BatchSummary

# Exit codes per sysexits.h convention (from RESEARCH.md)
EXIT_SUCCESS = 0
EXIT_PARTIAL = 75      # EX_TEMPFAIL: Some documents failed
EXIT_NO_INPUT = 66     # EX_NOINPUT: No documents found
EXIT_INPUT_ERROR = 64  # EX_USAGE: Bad input path

# Rich consoles
console = Console(stderr=True)


def _format_duration(seconds: float) -> str:
    """Format duration as human-readable string."""
    td = timedelta(seconds=seconds)
    if td.total_seconds() < 60:
        return f"{td.total_seconds():.1f}s"
    elif td.total_seconds() < 3600:
        minutes = int(td.total_seconds() // 60)
        secs = int(td.total_seconds() % 60)
        return f"{minutes}m {secs}s"
    else:
        hours = int(td.total_seconds() // 3600)
        minutes = int((td.total_seconds() % 3600) // 60)
        return f"{hours}h {minutes}m"


def _print_summary(summary: BatchSummary, quiet: bool = False) -> None:
    """Print batch processing summary."""
    console.print()
    console.print("[bold]Batch Processing Complete[/bold]")
    console.print()
    console.print(f"  [cyan]Documents:[/cyan] {summary.total_documents} total")
    console.print(f"    - Processed: [green]{summary.processed}[/green]")
    console.print(f"    - Skipped:   [yellow]{summary.skipped}[/yellow] (already processed)")
    if summary.failed > 0:
        console.print(f"    - Failed:    [red]{summary.failed}[/red]")
    console.print()
    console.print(f"  [cyan]Terms extracted:[/cyan] {summary.total_terms:,}")
    console.print(f"  [cyan]Duration:[/cyan] {_format_duration(summary.duration_seconds)}")

    if summary.errors and not quiet:
        console.print()
        console.print("[bold red]Errors:[/bold red]")
        for error in summary.errors[:10]:  # Show first 10
            console.print(f"  - {error}")
        if len(summary.errors) > 10:
            console.print(f"  ... and {len(summary.errors) - 10} more")


def _write_error_log(summary: BatchSummary, output_dir: Path) -> None:
    """Write errors to log file if any failures occurred."""
    if not summary.errors:
        return

    log_path = output_dir / "batch-errors.log"
    with open(log_path, "w", encoding="utf-8") as f:
        f.write(f"Batch Processing Errors\n")
        f.write(f"=======================\n\n")
        for error in summary.errors:
            f.write(f"- {error}\n")

    console.print(f"\n[yellow]Error log written to: {log_path}[/yellow]")


def batch_command(
    input_dir: Path = typer.Argument(
        ...,
        help="Directory containing PDF/EPUB documents to process",
        exists=True,
        file_okay=False,
        dir_okay=True,
    ),
    output_dir: Optional[Path] = typer.Option(
        None,
        "--output",
        "-o",
        help="Output directory for .vocab.json files (default: input_dir/output)",
    ),
    workers: int = typer.Option(
        0,
        "--workers",
        "-w",
        help="Number of parallel workers (0 = auto-detect based on CPU cores)",
    ),
    force: bool = typer.Option(
        False,
        "--force",
        "-f",
        help="Reprocess all documents, ignoring manifest",
    ),
    quiet: bool = typer.Option(
        False,
        "--quiet",
        "-q",
        help="Quiet mode: show only errors and final summary",
    ),
    blocklist: Optional[Path] = typer.Option(
        None,
        "--blocklist",
        "-b",
        help="Path to IP blocklist JSON (default: data/ip-blocklist.json)",
    ),
) -> None:
    """Process all documents in a folder with parallel execution.

    Discovers PDF and EPUB files in INPUT_DIR, processes them in parallel,
    and outputs .vocab.json files. Progress is displayed with a Rich progress
    bar. Interrupted runs automatically resume from where they stopped.

    Examples:
        corpora batch ./documents
        corpora batch ./documents -o ./vocab --workers 4
        corpora batch ./documents --force --quiet
    """
    # Resolve output directory
    if output_dir is None:
        output_dir = input_dir / "output"

    # Auto-detect workers if not specified
    if workers <= 0:
        workers = min(8, os.cpu_count() or 4)

    # Default blocklist path
    if blocklist is None:
        default_blocklist = Path("data/ip-blocklist.json")
        if default_blocklist.exists():
            blocklist = default_blocklist

    # Create config
    config = BatchConfig(
        input_dir=input_dir,
        output_dir=output_dir,
        max_workers=workers,
        force_reprocess=force,
        blocklist_path=blocklist,
    )

    # Discover documents first to show count
    processor = BatchProcessor(config)
    documents = processor.discover_documents()

    if not documents:
        console.print(f"[yellow]No PDF or EPUB files found in {input_dir}[/yellow]")
        raise typer.Exit(EXIT_NO_INPUT)

    if not quiet:
        console.print(f"[cyan]Found {len(documents)} document(s) in {input_dir}[/cyan]")
        console.print(f"[cyan]Output directory: {output_dir}[/cyan]")
        console.print(f"[cyan]Workers: {workers}[/cyan]")
        if force:
            console.print("[yellow]Force mode: reprocessing all documents[/yellow]")
        console.print()

    # Process with progress display
    results = []

    if quiet:
        # Quiet mode: no progress bar, just process
        for result in processor.process():
            results.append(result)
            if result.status == DocumentStatus.FAILED:
                console.print(f"[red]FAILED:[/red] {result.source_path.name}: {result.error}")
    else:
        # Normal mode: Rich progress bar
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task("Processing documents...", total=len(documents))

            def on_complete(result: DocumentResult) -> None:
                results.append(result)
                progress.update(task, advance=1)

                # Show per-document result
                if result.status == DocumentStatus.SUCCESS:
                    progress.console.print(
                        f"  [green]OK[/green] {result.source_path.name} "
                        f"({result.term_count} terms, {result.duration_seconds:.1f}s)"
                    )
                elif result.status == DocumentStatus.SKIPPED:
                    progress.console.print(
                        f"  [yellow]SKIP[/yellow] {result.source_path.name} (already processed)"
                    )
                else:
                    progress.console.print(
                        f"  [red]FAIL[/red] {result.source_path.name}: {result.error}"
                    )

            # Create new processor with callback
            processor = BatchProcessor(config, on_document_complete=on_complete)

            # Consume the generator
            for _ in processor.process():
                pass

    # Generate summary
    processed = sum(1 for r in results if r.status == DocumentStatus.SUCCESS)
    skipped = sum(1 for r in results if r.status == DocumentStatus.SKIPPED)
    failed = sum(1 for r in results if r.status == DocumentStatus.FAILED)
    total_terms = sum(r.term_count for r in results)
    errors = [f"{r.source_path.name}: {r.error}" for r in results if r.error]
    duration = sum(r.duration_seconds for r in results)

    summary = BatchSummary(
        total_documents=len(results),
        processed=processed,
        skipped=skipped,
        failed=failed,
        total_terms=total_terms,
        duration_seconds=duration,
        errors=errors,
    )

    # Print summary
    _print_summary(summary, quiet)

    # Write error log if any failures
    if summary.errors:
        _write_error_log(summary, output_dir)

    # Exit code based on results
    if failed > 0:
        raise typer.Exit(EXIT_PARTIAL)
    raise typer.Exit(EXIT_SUCCESS)
```

Then update src/corpora/cli/main.py to register the command:

```python
from corpora.cli.batch import batch_command
# ... existing imports ...

# Add to registrations:
app.command(name="batch")(batch_command)
```
  </action>
  <verify>python -c "from corpora.cli.batch import batch_command; print('CLI OK')" && python -m corpora batch --help</verify>
  <done>`corpora batch --help` shows command with all options (--workers, --quiet, --force, --output, --blocklist)</done>
</task>

<task type="auto">
  <name>Task 2: Add comprehensive tests for batch module</name>
  <files>tests/test_batch.py</files>
  <action>
Create tests/test_batch.py with coverage for models, processor, and CLI:

```python
"""Tests for the batch processing module.

Tests cover:
- Batch models (DocumentResult, BatchConfig, BatchSummary)
- BatchProcessor document discovery
- BatchProcessor manifest-based resumability
- CLI command integration
"""

import json
import os
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest
from typer.testing import CliRunner

from corpora.batch import (
    BatchConfig,
    BatchProcessor,
    BatchSummary,
    DocumentResult,
    DocumentStatus,
)
from corpora.cli.main import app


runner = CliRunner()


# ============================================================================
# Model Tests
# ============================================================================

class TestDocumentResult:
    """Tests for DocumentResult model."""

    def test_success_result(self):
        result = DocumentResult(
            source_path=Path("test.pdf"),
            status=DocumentStatus.SUCCESS,
            term_count=42,
            vocab_path=Path("test.vocab.json"),
            duration_seconds=1.5,
        )
        assert result.status == DocumentStatus.SUCCESS
        assert result.term_count == 42
        assert result.error is None

    def test_failed_result(self):
        result = DocumentResult(
            source_path=Path("test.pdf"),
            status=DocumentStatus.FAILED,
            error="Parse error",
        )
        assert result.status == DocumentStatus.FAILED
        assert result.error == "Parse error"
        assert result.term_count == 0

    def test_skipped_result(self):
        result = DocumentResult(
            source_path=Path("test.pdf"),
            status=DocumentStatus.SKIPPED,
        )
        assert result.status == DocumentStatus.SKIPPED


class TestBatchConfig:
    """Tests for BatchConfig model."""

    def test_default_workers(self, tmp_path):
        config = BatchConfig(
            input_dir=tmp_path,
            output_dir=tmp_path / "output",
        )
        assert config.max_workers == 4
        assert config.force_reprocess is False

    def test_custom_workers(self, tmp_path):
        config = BatchConfig(
            input_dir=tmp_path,
            output_dir=tmp_path / "output",
            max_workers=8,
        )
        assert config.max_workers == 8

    def test_worker_bounds(self, tmp_path):
        with pytest.raises(ValueError):
            BatchConfig(
                input_dir=tmp_path,
                output_dir=tmp_path / "output",
                max_workers=0,
            )

        with pytest.raises(ValueError):
            BatchConfig(
                input_dir=tmp_path,
                output_dir=tmp_path / "output",
                max_workers=20,
            )


class TestBatchSummary:
    """Tests for BatchSummary model."""

    def test_summary_creation(self):
        summary = BatchSummary(
            total_documents=10,
            processed=7,
            skipped=2,
            failed=1,
            total_terms=500,
            duration_seconds=30.5,
            errors=["test.pdf: error"],
        )
        assert summary.total_documents == 10
        assert summary.processed == 7
        assert summary.failed == 1


# ============================================================================
# Processor Tests
# ============================================================================

class TestBatchProcessor:
    """Tests for BatchProcessor class."""

    def test_discover_documents(self, tmp_path):
        """Test document discovery finds PDF and EPUB files."""
        # Create test files
        (tmp_path / "doc1.pdf").touch()
        (tmp_path / "doc2.epub").touch()
        (tmp_path / "doc3.txt").touch()  # Should be ignored
        (tmp_path / "doc4.PDF").touch()  # Case sensitivity

        config = BatchConfig(
            input_dir=tmp_path,
            output_dir=tmp_path / "output",
        )
        processor = BatchProcessor(config)
        docs = processor.discover_documents()

        # Should find pdf and epub, not txt
        names = [d.name for d in docs]
        assert "doc1.pdf" in names
        assert "doc2.epub" in names
        assert "doc3.txt" not in names

    def test_discover_empty_directory(self, tmp_path):
        """Test discovery returns empty list for empty directory."""
        config = BatchConfig(
            input_dir=tmp_path,
            output_dir=tmp_path / "output",
        )
        processor = BatchProcessor(config)
        docs = processor.discover_documents()
        assert docs == []

    def test_manifest_skip_unchanged(self, tmp_path):
        """Test that unchanged documents are skipped."""
        # Create a PDF file
        pdf_path = tmp_path / "test.pdf"
        pdf_path.write_bytes(b"%PDF-1.4 test content")

        # Create a manifest that already has this file
        manifest_path = tmp_path / ".corpora-manifest.json"

        # We need to compute the hash the same way the manifest does
        from corpora.output.vocab_writer import compute_file_hash
        file_hash = compute_file_hash(pdf_path)

        manifest_data = {
            "schema_version": "1.0",
            "last_updated": "2024-01-01T00:00:00",
            "documents": {
                str(pdf_path): {
                    "source_path": str(pdf_path),
                    "source_hash": file_hash,
                    "vocab_path": str(tmp_path / "test.vocab.json"),
                    "processed_at": "2024-01-01T00:00:00",
                    "term_count": 10,
                }
            }
        }
        manifest_path.write_text(json.dumps(manifest_data))

        config = BatchConfig(
            input_dir=tmp_path,
            output_dir=tmp_path / "output",
            force_reprocess=False,
        )
        processor = BatchProcessor(config, manifest_path=manifest_path)

        results = list(processor.process())
        assert len(results) == 1
        assert results[0].status == DocumentStatus.SKIPPED

    def test_force_reprocess_ignores_manifest(self, tmp_path):
        """Test that --force reprocesses even with existing manifest entry."""
        # Create a PDF file
        pdf_path = tmp_path / "test.pdf"
        pdf_path.write_bytes(b"%PDF-1.4 test content")

        # Create manifest (same as above)
        manifest_path = tmp_path / ".corpora-manifest.json"
        from corpora.output.vocab_writer import compute_file_hash
        file_hash = compute_file_hash(pdf_path)

        manifest_data = {
            "schema_version": "1.0",
            "last_updated": "2024-01-01T00:00:00",
            "documents": {
                str(pdf_path): {
                    "source_path": str(pdf_path),
                    "source_hash": file_hash,
                    "vocab_path": str(tmp_path / "test.vocab.json"),
                    "processed_at": "2024-01-01T00:00:00",
                    "term_count": 10,
                }
            }
        }
        manifest_path.write_text(json.dumps(manifest_data))

        config = BatchConfig(
            input_dir=tmp_path,
            output_dir=tmp_path / "output",
            force_reprocess=True,  # Force!
        )

        # Mock the processing to avoid real API calls
        with patch.object(BatchProcessor, '_process_single_document') as mock_process:
            mock_process.return_value = DocumentResult(
                source_path=pdf_path,
                status=DocumentStatus.SUCCESS,
                term_count=5,
                vocab_path=tmp_path / "output" / "test.vocab.json",
            )

            processor = BatchProcessor(config, manifest_path=manifest_path)
            results = list(processor.process())

            # Should process (not skip) because force=True
            assert len(results) == 1
            assert results[0].status == DocumentStatus.SUCCESS
            mock_process.assert_called()


# ============================================================================
# CLI Tests
# ============================================================================

class TestBatchCLI:
    """Tests for batch CLI command."""

    def test_help(self):
        """Test that help displays correctly."""
        result = runner.invoke(app, ["batch", "--help"])
        assert result.exit_code == 0
        assert "Process all documents in a folder" in result.output
        assert "--workers" in result.output
        assert "--quiet" in result.output
        assert "--force" in result.output

    def test_no_documents_found(self, tmp_path):
        """Test exit code when no documents found."""
        result = runner.invoke(app, ["batch", str(tmp_path)])
        assert result.exit_code == 66  # EXIT_NO_INPUT
        assert "No PDF or EPUB files found" in result.output

    def test_invalid_directory(self, tmp_path):
        """Test error on non-existent directory."""
        result = runner.invoke(app, ["batch", str(tmp_path / "nonexistent")])
        assert result.exit_code != 0

    @patch.object(BatchProcessor, 'process')
    @patch.object(BatchProcessor, 'discover_documents')
    def test_quiet_mode(self, mock_discover, mock_process, tmp_path):
        """Test quiet mode shows minimal output."""
        # Setup mocks
        pdf_path = tmp_path / "test.pdf"
        pdf_path.touch()

        mock_discover.return_value = [pdf_path]
        mock_process.return_value = iter([
            DocumentResult(
                source_path=pdf_path,
                status=DocumentStatus.SUCCESS,
                term_count=10,
                duration_seconds=1.0,
            )
        ])

        result = runner.invoke(app, ["batch", str(tmp_path), "--quiet"])

        # Should show summary but not progress bar details
        assert "Batch Processing Complete" in result.output

    @patch.object(BatchProcessor, 'process')
    @patch.object(BatchProcessor, 'discover_documents')
    def test_partial_failure_exit_code(self, mock_discover, mock_process, tmp_path):
        """Test exit code 75 on partial failure."""
        pdf1 = tmp_path / "good.pdf"
        pdf2 = tmp_path / "bad.pdf"
        pdf1.touch()
        pdf2.touch()

        mock_discover.return_value = [pdf1, pdf2]
        mock_process.return_value = iter([
            DocumentResult(
                source_path=pdf1,
                status=DocumentStatus.SUCCESS,
                term_count=10,
            ),
            DocumentResult(
                source_path=pdf2,
                status=DocumentStatus.FAILED,
                error="Parse error",
            ),
        ])

        result = runner.invoke(app, ["batch", str(tmp_path), "--quiet"])
        assert result.exit_code == 75  # EXIT_PARTIAL
```

Run tests to verify:
```bash
pytest tests/test_batch.py -v
```

Ensure all tests pass. If there are import issues or missing dependencies, fix them.
  </action>
  <verify>pytest tests/test_batch.py -v</verify>
  <done>All batch tests pass, covering models, processor discovery, manifest-based resume, force mode, and CLI commands</done>
</task>

</tasks>

<verification>
1. `corpora batch --help` shows all options
2. `corpora batch ./test-docs` processes documents (with real docs)
3. Progress bar displays during processing
4. Interrupted run resumes (skip already processed)
5. `--force` reprocesses all
6. `--quiet` shows only summary
7. Exit code 75 on partial failure
8. pytest tests/test_batch.py passes
</verification>

<success_criteria>
- `corpora batch <folder>` command works with progress display
- --workers N controls parallelism (0 = auto-detect)
- --quiet shows only errors and final summary
- --force ignores manifest and reprocesses all
- Interrupted runs resume automatically
- Exit codes follow sysexits.h (0 success, 75 partial, 66 no input)
- Error log written on failures
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-batch-processing/04-02-SUMMARY.md`
</output>
